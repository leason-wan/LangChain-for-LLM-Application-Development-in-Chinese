1
00:00:01,000 --> 00:00:15,000
使用llm构建的最常见的复杂应用程序之一是一个系统，可以在文档上方或关于文档回答问题。
One of the most common complex applications that people are building using an llm is a system that can answer questions on top of or about a document.

2
00:00:15,000 --> 00:00:24,000
因此，给定从PDF文件、网页或某些公司的内部文档收集中提取的文本，
So given a piece of text may be extracted from PDF file or from a webpage or from some company's intranet internal document collection,

3
00:00:24,000 --> 00:00:33,000
您可以使用llm回答有关这些文档内容的问题，以帮助用户获得更深入的理解并获得所需的信息吗？
can you use an llm to answer questions about the content of those documents to help users gain a deeper understanding and get access to the information that they need?

4
00:00:33,000 --> 00:00:39,000
这真的很强大，因为它开始将这些语言模型与它们最初没有接受培训的数据结合起来。
This is really powerful because it starts to combine these language models with data that they weren't originally trained on.

5
00:00:39,000 --> 00:00:42,000
因此，它使它们更加灵活和适应您的用例。
So it makes them much more flexible and adaptable to your use case.

6
00:00:42,000 --> 00:00:48,000
这也非常令人兴奋，因为我们将开始超越语言模型、提示和输出解析器，
It's also really exciting because we'll start to move beyond language models, prompts, and output parsers,

7
00:00:48,000 --> 00:00:54,000
并开始引入链式的一些关键组件，例如嵌入模型和向量存储。
and start introducing some more of the key components of link chain, such as embedding models and vector stores.

8
00:00:54,000 --> 00:00:58,000
正如安德鲁所提到的，这是我们拥有的最受欢迎的链之一，所以我希望你很兴奋。
As Andrew mentioned, this is one of the more popular chains that we've got, so I hope you're excited.

9
00:00:58,000 --> 00:01:03,000
实际上，嵌入和向量存储是一些最强大的现代技术。
In fact, embeddings and vector stores are some of the most powerful modern techniques.

10
00:01:03,000 --> 00:01:08,000
因此，如果您还没有看到它们，那么了解它们非常值得。
So if you have not seen them yet, they are very much worth learning about.

11
00:01:08,000 --> 00:01:10,000
那么，让我们开始吧。
So with that, let's dive in.

12
00:01:10,000 --> 00:01:11,000
开始吧。
Let's do it.

13
00:01:11,000 --> 00:01:16,000
因此，我们将从像往常一样导入环境变量开始。
So we're going to start by importing the environment variables as we always do.

14
00:01:16,000 --> 00:01:20,000
现在，我们将导入一些在构建此链时将有所帮助的东西。
Now we're going to import some things that will help us when building this chain.

15
00:01:20,000 --> 00:01:22,000
我们将导入检索QA链。
We're going to import the retrieval QA chain.

16
00:01:22,000 --> 00:01:24,000
这将在一些文档上进行检索。
This will do retrieval over some documents.

17
00:01:24,000 --> 00:01:28,000
我们将导入我们最喜欢的聊天Open AI语言模型。
We're going to import our favorite chat open AI language model.

18
00:01:28,000 --> 00:01:29,000
我们将导入文档加载器。
We're going to import a document loader.

19
00:01:29,000 --> 00:01:34,000
这将用于加载一些专有数据，我们将与语言模型结合使用。
This is going to be used to load some proprietary data that we're going to combine with the language model.

20
00:01:34,000 --> 00:01:36,000
在这种情况下，它将在CSV中。
In this case, it's going to be in a CSV.

21
00:01:36,000 --> 00:01:39,000
因此，我们将导入CSV加载器。
So we're going to import the CSV loader.

22
00:01:39,000 --> 00:01:41,000
最后，我们将导入向量存储。
Finally, we're going to import a vector store.

23
00:01:41,000 --> 00:01:45,000
有许多不同类型的向量存储，我们将在稍后介绍它们的确切含义。
There are many different types of vector stores, and we'll cover what exactly these are later on.

24
00:01:45,000 --> 00:01:49,000
但是，我们将从Dock Array内存搜索向量存储开始。
But we're going to get started with the Dock Array in-memory search vector store.

25
00:01:49,000 --> 00:01:51,000
这非常好，因为它是一个内存向量存储，
This is really nice because it's an in-memory vector store,

26
00:01:51,000 --> 00:01:55,000
并且不需要连接到任何外部数据库，
 
and it doesn't require connecting to an external database of any kind,

27
00:01:55,000 --> 00:01:57,000
所以它使得入门变得非常容易。
so it makes it really easy to get started.

28
00:01:57,000 --> 00:01:59,000
我们还将导入显示和markdown两个常见的在Jupyter Notebooks中显示信息的工具。
We're also going to import display and markdown,

29
00:01:59,000 --> 00:02:04,000
我们提供了一个户外服装的CSV文件，我们将使用它与语言模型结合使用。
two common utilities for displaying information in Jupyter Notebooks.

30
00:02:04,000 --> 00:02:10,000
在这里，我们将使用该文件的路径初始化一个加载器，即CSV加载器。
We've provided a CSV of outdoor clothing that we're going to use to combine with the language model.

31
00:02:10,000 --> 00:02:18,000
接下来，我们将导入一个索引，即向量存储索引创建器。
Here we're going to initialize a loader, the CSV loader, with a path to this file.

32
00:02:18,000 --> 00:02:22,000
这将帮助我们非常容易地创建一个向量存储。
We're next going to import an index, the vector store index creator.

33
00:02:22,000 --> 00:02:26,000
如下所示，只需要几行代码就可以创建它。
This will help us create a vector store really easily.

34
00:02:26,000 --> 00:02:34,000
为了创建它，我们将指定两件事。
As we can see below, it will only be a few lines of code to create this.

35
00:02:34,000 --> 00:02:37,000
首先，我们将指定向量存储类。
To create it, we're going to specify two things.

36
00:02:37,000 --> 00:02:40,000
如前所述，我们将使用这个向量存储，
First, we're going to specify the vector store class.

37
00:02:40,000 --> 00:02:46,000
因为它是一个特别容易入门的向量存储。
As mentioned before, we're going to use this vector store,

38
00:02:46,000 --> 00:02:49,000
创建完成后，我们将从加载器中调用，
as it's a particularly easy one to get started with.

39
00:02:49,000 --> 00:02:51,000
它接受一个文档加载器列表。
After it's been created, we're then going to call from loaders,

40
00:02:51,000 --> 00:02:58,000
我们只有一个我们真正关心的加载器，所以这就是我们在这里传递的。
which takes in a list of document loaders.

41
00:02:58,000 --> 00:03:02,000
现在它已经被创建了，我们可以开始询问它的问题了。
We've only got one loader that we really care about, so that's what we're passing in here.

42
00:03:02,000 --> 00:03:07,000
下面我们将介绍发生了什么，所以现在不要担心这个。
It's now been created, and we can start to ask questions about it.

43
00:03:07,000 --> 00:03:09,000
在这里我们将从一个查询开始。
Below we'll cover what exactly happened under the hood, so let's not worry about that for now.

44
00:03:09,000 --> 00:03:17,000
然后我们将使用索引查询创建一个响应，并传入这个查询。
Here we'll start with a query.

45
00:03:17,000 --> 00:03:21,000
同样，我们将在下面介绍发生了什么。
We'll then create a response using index query and pass in this query.

46
00:03:21,000 --> 00:03:30,000
现在，我们只需要等待它的响应。
Again, we'll cover what's going on under the hood down below.

47
00:03:30,000 --> 00:03:34,000
完成后，我们现在可以看看到底返回了什么。
For now, we'll just wait for it to respond.

48
00:03:34,000 --> 00:03:41,000
我们得到了一个Markdown表格，其中包含所有带有防晒衣的衬衫的名称和描述。
After it finishes, we can now take a look at what exactly was returned.

49
00:03:41,000 --> 00:03:45,000
我们还得到了一个语言模型提供的不错的小总结。
We've gotten back a table in Markdown with names and descriptions for all shirts with sun protection.

50
00:03:45,000 --> 00:03:48,000
所以我们已经介绍了如何在您的文档中进行问答，
We've also got a nice little summary that the language model has provided us.

51
00:03:48,000 --> 00:03:52,000
但是到底在幕后发生了什么呢？
So we've gone over how to do question answering over your documents,

52
00:03:52,000 --> 00:03:54,000
首先，让我们考虑一般的想法。
but what exactly is going on underneath the hood?

53
00:03:54,000 --> 00:03:58,000
我们想要使用语言模型并将其与我们的许多文档结合使用，
First, let's think about the general idea.

54
00:03:58,000 --> 00:04:03,000
但是有一个关键问题。语言模型一次只能检查几千个单词。
 
We want to use language models and combine it with a lot of our documents,

55
00:04:03,000 --> 00:04:10,000
如果我们有非常大的文档，如何让语言模型回答关于其中所有内容的问题呢？
but there's a key issue. Language models can only inspect a few thousand words at a time.

56
00:04:10,000 --> 00:04:14,000
这就是嵌入和向量存储发挥作用的地方。
So if we have really large documents, how can we get the language model to answer questions about everything that's in there?

57
00:04:14,000 --> 00:04:17,000
首先，让我们谈谈嵌入。
This is where embeddings and vector stores come into play.

58
00:04:17,000 --> 00:04:21,000
嵌入为文本片段创建数字表示。
First, let's talk about embeddings.

59
00:04:21,000 --> 00:04:27,000
这种数字表示捕捉了它所运行的文本片段的语义含义。
Embeddings create numerical representations for pieces of text.

60
00:04:27,000 --> 00:04:31,000
相似内容的文本片段将具有相似的向量。
This numerical representation captures the semantic meaning of the piece of text that it's been run over.

61
00:04:31,000 --> 00:04:35,000
这使我们可以在向量空间中比较文本片段。
Pieces of text with similar content will have similar vectors.

62
00:04:35,000 --> 00:04:38,000
在下面的示例中，我们可以看到我们有三个句子。
This lets us compare pieces of text in the vector space.

63
00:04:38,000 --> 00:04:43,000
前两个是关于宠物的，而第三个是关于汽车的。
In the example below, we can see that we have three sentences.

64
00:04:43,000 --> 00:04:46,000
如果我们看一下数字空间中的表示，
The first two are about pets, while the third is about a car.

65
00:04:46,000 --> 00:04:54,000
我们可以看到当我们比较与宠物句子相对应的文本片段上的两个向量时，它们非常相似。
If we look at the representation in the numeric space,

66
00:04:54,000 --> 00:04:58,000
而如果我们将其与谈论汽车的那个进行比较，它们根本不相似。
we can see that when we compare the two vectors on the pieces of text corresponding to the sentences about pets, they're very similar.

67
00:04:58,000 --> 00:05:02,000
这将让我们轻松地找出哪些文本片段彼此相似，
While if we compare it to the one that talks about a car, they're not similar at all.

68
00:05:02,000 --> 00:05:10,000
这在我们考虑要包含哪些文本片段传递给语言模型以回答问题时非常有用。
This will let us easily figure out which pieces of text are like each other,

69
00:05:10,000 --> 00:05:13,000
我们要介绍的下一个组件是向量数据库。
which will be very useful as we think about which pieces of text we want to include when passing to the language model to answer a question.

70
00:05:13,000 --> 00:05:18,000
向量数据库是存储我们在上一步中创建的这些向量表示的一种方式。
The next component that we're going to cover is the vector database.

71
00:05:18,000 --> 00:05:24,000
我们创建这个向量数据库的方式是用来自传入文档的文本块填充它。
A vector database is a way to store these vector representations that we created in the previous step.

72
00:05:24,000 --> 00:05:28,000
当我们获得一个大的传入文档时，我们首先将其分成较小的块。
The way that we create this vector database is we populate it with chunks of text coming from incoming documents.

73
00:05:28,000 --> 00:05:33,000
这有助于创建比原始文档小的文本片段，
When we get a big incoming document, we're first going to break it up into smaller chunks.

74
00:05:33,000 --> 00:05:37,000
这很有用，因为我们可能无法将整个文档传递给语言模型。
This helps create pieces of text that are smaller than the original document,

75
00:05:37,000 --> 00:05:43,000
因此，我们想创建这些小块，以便只传递最相关的块给语言模型。
which is useful because we may not be able to pass the whole document to the language model.

76
00:05:43,000 --> 00:05:48,000
然后，我们为每个这些块创建一个嵌入，然后将它们存储在向量数据库中。
So we want to create these small chunks so we can only pass the most relevant ones to the language model.

77
00:05:48,000 --> 00:05:51,000
这就是我们创建索引时发生的事情。
We then create an embedding for each of these chunks, and then we store those in a vector database.

78
00:05:51,000 --> 00:05:58,000
现在我们有了这个索引，我们可以在运行时使用它来查找与传入查询最相关的文本片段。
That's what happens when we create the index.

79
00:05:58,000 --> 00:06:02,000
当查询进来时，我们首先为该查询创建一个嵌入。
 
Now that we've got this index, we can use it during runtime to find the pieces of text most relevant to an incoming query.

80
00:06:02,000 --> 00:06:07,000
然后我们将其与向量数据库中的所有向量进行比较，并选择最相似的n个。
When a query comes in, we first create an embedding for that query.

81
00:06:07,000 --> 00:06:14,000
然后将它们返回，我们可以将它们传递到语言模型中，以获得最终答案。
We then compare it to all the vectors in the vector database, and we pick the n most similar.

82
00:06:14,000 --> 00:06:17,000
因此，我们创建了这个链，只需要几行代码。
These are then returned, and we can pass those in the prompt to the language model to get back a final answer.

83
00:06:17,000 --> 00:06:19,000
这对于快速入门非常有用。
So above, we created this chain and only a few lines of code.

84
00:06:19,000 --> 00:06:25,000
好的，现在让我们逐步进行，并了解底层到底发生了什么。
That's great for getting started quickly.

85
00:06:25,000 --> 00:06:27,000
第一步与上面类似。
Well, let's now do it a bit more step by step and understand what exactly is going on under the hood.

86
00:06:27,000 --> 00:06:36,000
我们将创建一个文档加载器，从包含我们要进行问题回答的所有产品描述的CSV中加载。
The first step is similar to above.

87
00:06:36,000 --> 00:06:41,000
然后我们可以从这个文档加载器中加载文档。
We're going to create a document loader, loading from that CSV with all the descriptions of the products that we want to do question answering over.

88
00:06:41,000 --> 00:06:50,000
如果我们查看单个文档，我们可以看到每个文档对应于CSV中的一个产品。
We can then load documents from this document loader.

89
00:06:50,000 --> 00:06:53,000
之前，我们谈到了创建块。
If we look at the individual documents, we can see that each document corresponds to one of the products in the CSV.

90
00:06:53,000 --> 00:07:01,000
因为这些文档已经非常小了，所以我们实际上不需要在这里进行任何分块，因此我们可以直接创建嵌入。
Previously, we talked about creating chunks.

91
00:07:01,000 --> 00:07:05,000
要创建嵌入，我们将使用OpenAI的嵌入类。
Because these documents are already so small, we actually don't need to do any chunking here, and so we can create embeddings directly.

92
00:07:05,000 --> 00:07:08,000
我们可以在这里导入它并初始化它。
To create embeddings, we're going to use OpenAI's embedding class.

93
00:07:08,000 --> 00:07:21,000
如果我们想看看这些嵌入是如何工作的，我们实际上可以看一下嵌入特定文本时会发生什么。
We can import it and initialize it here.

94
00:07:21,000 --> 00:07:26,000
让我们使用嵌入对象上的嵌入查询方法为特定文本创建嵌入。
If we want to see what these embeddings do, we can actually take a look at what happens when we embed a particular piece of text.

95
00:07:26,000 --> 00:07:31,000
在这种情况下，句子是“嗨，我的名字是哈里森”。
Let's use the embed query method on the embeddings object to create embeddings for a particular piece of text.

96
00:07:31,000 --> 00:07:41,000
如果我们查看这个嵌入，我们可以看到有超过一千个不同的元素。
In this case, the sentence, hi, my name is Harrison.

97
00:07:41,000 --> 00:07:44,000
每个元素都是不同的数字值。
If we take a look at this embedding, we can see that there are over a thousand different elements.

98
00:07:44,000 --> 00:07:51,000
组合起来，这就创建了这段文本的总体数值表示。
Each of these elements is a different numerical value.

99
00:07:51,000 --> 00:07:58,000
我们想为刚刚加载的所有文本创建嵌入，然后我们还想将它们存储在向量存储中。
Combined, this creates the overall numerical representation for this piece of text.

100
00:07:58,000 --> 00:08:03,000
我们可以使用向量存储上的from documents方法来实现这一点。
We want to create embeddings for all the pieces of text that we just loaded, and then we also want to store them in a vector store.

101
00:08:03,000 --> 00:08:12,000
该方法接受文档列表、嵌入对象，然后我们将创建一个总体向量存储。
We can do that by using the from documents method on the vector store.

102
00:08:12,000 --> 00:08:18,000
现在，我们可以使用这个向量存储来查找与传入查询类似的文本。
This method takes in a list of documents, an embedding object, and then we'll create an overall vector store.

103
00:08:18,000 --> 00:08:23,000
因此，让我们看一下查询，请建议一件带有防晒功能的衬衫。
 
We can now use this vector store to find pieces of text similar to an incoming query.

104
00:08:23,000 --> 00:08:36,000
如果我们在向量存储中使用相似性搜索方法并传入一个查询，我们将得到一个文档列表。
So let's look at the query, please suggest a shirt with sunblocking.

105
00:08:36,000 --> 00:08:48,000
我们可以看到它返回了四个文档，如果我们看第一个文档，我们可以看到它确实是一件关于防晒的衬衫。
If we use the similarity search method on the vector store and pass in a query, we will get back a list of documents.

106
00:08:48,000 --> 00:08:52,000
那么我们如何使用它来回答我们自己的文档问题呢？
We can see that it returns four documents, and if we look at the first one, we can see that it is indeed a shirt about sunblocking.

107
00:08:52,000 --> 00:08:57,000
首先，我们需要从这个向量存储中创建一个检索器。
So how do we use this to do question answering over our own documents?

108
00:08:57,000 --> 00:09:03,000
检索器是一个通用接口，可以由任何接受查询并返回文档的方法支持。
First, we need to create a retriever from this vector store.

109
00:09:03,000 --> 00:09:11,000
向量存储和嵌入是一种这样的方法，尽管有许多不同的方法，有些不太先进，有些更先进。
A retriever is a generic interface that can be underpinned by any method that takes in a query and returns documents.

110
00:09:11,000 --> 00:09:20,000
接下来，因为我们想要进行文本生成并返回自然语言响应，我们将导入一个语言模型，我们将使用聊天开放AI。
Vector stores and embeddings are one such method to do so, although there are plenty of different methods, some less advanced, some more advanced.

111
00:09:20,000 --> 00:09:28,000
如果我们手动进行此操作，我们将合并文档中的所有页面内容到一个变量中。
Next, because we want to do text generation and return a natural language response, we're going to import a language model and we're going to use chat open AI.

112
00:09:28,000 --> 00:09:37,000
因此，我们会做一些像这样的事情，将所有页面内容连接到一个变量中。
If we were doing this by hand, what we would do is we would combine the documents into a single piece of text.

113
00:09:37,000 --> 00:09:48,000
然后，我们将传递此变量或问题的变体，例如请列出所有具有防晒功能的衬衫并在Markdown表格中总结每个衬衫的语言模型。
So we'd do something like this, where we join all the page content in the documents into a variable.

114
00:09:48,000 --> 00:09:55,000
如果我们在此处打印响应，我们可以看到我们得到了一个表格，正如我们所要求的那样。
And then we'd pass this variable or a variant on the question, like please list all your shirts with sun protection and a table with markdown and summarize each one into the language model.

115
00:09:55,000 --> 00:09:59,000
所有这些步骤都可以用LangChain链封装起来。
And if we print out the response here, we can see that we get back a table exactly as we asked for.

116
00:09:59,000 --> 00:10:02,000
因此，我们可以创建一个检索QA链。
All of those steps can be encapsulated with the LangChain chain.

117
00:10:02,000 --> 00:10:06,000
这将进行检索，然后对检索到的文档进行问题回答。
So here we can create a retrieval QA chain.

118
00:10:06,000 --> 00:10:09,000
要创建这样的链，我们将传入几个不同的东西。
This does retrieval and then does question answering over the retrieved documents.

119
00:10:09,000 --> 00:10:12,000
首先，我们将传入语言模型。
To create such a chain, we'll pass in a few different things.

120
00:10:12,000 --> 00:10:15,000
这将用于在最后进行文本生成。
First, we'll pass in the language model.

121
00:10:15,000 --> 00:10:17,000
接下来，我们将传入链类型。
This will be used for doing the text generation at the end.

122
00:10:17,000 --> 00:10:18,000
我们将使用stuff。
Next, we'll pass in the chain type.

123
00:10:18,000 --> 00:10:25,000
这是最简单的方法，因为它只是将所有文档塞入上下文并对语言模型进行一次调用。
We're going to use stuff.

124
00:10:25,000 --> 00:10:32,000
还有一些其他方法可以用来进行问题回答，我可能会在最后提及，但我们不会详细讨论。
This is the simplest method as it just stuffs all the documents into context and makes one call to a language model.

125
00:10:32,000 --> 00:10:34,000
第三，我们将传入一个检索器。
There are a few other methods that you can use to do question answering that I'll maybe touch on at the end, but we're not going to look at in detail.

126
00:10:34,000 --> 00:10:38,000
我们上面创建的检索器只是一个获取文档的接口。
 
Third, we're going to pass in a retriever.

127
00:10:38,000 --> 00:10:41,000
这将用于获取文档并将其传递给语言模型。
The retriever we created above is just an interface for fetching documents.

128
00:10:41,000 --> 00:10:46,000
最后，我们将设置 verbose 等于 true。
This will be used to fetch documents and pass it to the language model.

129
00:10:46,000 --> 00:11:08,000
现在我们可以创建一个查询并在此查询上运行链。
And then finally, we're going to set verbose equals to true.

130
00:11:08,000 --> 00:11:14,000
当我们获得响应时，我们可以再次使用 display 和 markdown 实用程序显示它。
Now we can create a query and we can run the chain on this query.

131
00:11:14,000 --> 00:11:20,000
您可以在此暂停视频并尝试使用一堆不同的查询。
When we get the response, we can again display it using the display and markdown utilities.

132
00:11:20,000 --> 00:11:26,000
所以这就是您详细了解它的方式，但请记住，我们仍然可以轻松地使用我们上面的一行来完成它。
You can pause the video here and try it out with a bunch of different queries.

133
00:11:26,000 --> 00:11:30,000
因此，这两个东西等同于相同的结果。
So that's how you do it in detail, but remember that we can still do it pretty easily with just the one line that we had up above.

134
00:11:30,000 --> 00:11:32,000
这就是 LinkChain 的有趣之处。
So these two things equate to the same result.

135
00:11:32,000 --> 00:11:38,000
您可以在一行中完成它，也可以查看各个内容并将其分解为更详细的五个内容。
And that's part of the interesting stuff about LinkChain.

136
00:11:38,000 --> 00:11:44,000
五个更详细的内容让您设置更多关于正在发生的确切内容的细节，但一行代码很容易入手。
You can do it in one line or you can look at the individual things and break it down into five more detailed ones.

137
00:11:44,000 --> 00:11:48,000
所以由您决定如何继续前进。
The five more detailed ones let you set more specifics about what exactly is going on, but the one-liner is easy to get started.

138
00:11:48,000 --> 00:11:51,000
我们还可以在创建索引时自定义索引。
So up to you as to how you'd prefer to go forward.

139
00:11:51,000 --> 00:11:55,000
因此，如果您记得，当我们手动创建它时，我们指定了一个嵌入。
We can also customize the index when we're creating it.

140
00:11:55,000 --> 00:11:57,000
我们也可以在这里指定一个嵌入。
And so if you remember, when we created it by hand, we specified an embedding.

141
00:11:57,000 --> 00:12:01,000
这将使我们能够灵活地创建嵌入本身。
And we can specify an embedding here as well.

142
00:12:01,000 --> 00:12:06,000
我们还可以在此处替换向量存储器以获取不同类型的向量存储器。
And so this will give us flexibility over how the embeddings themselves are created.

143
00:12:06,000 --> 00:12:15,000
因此，在创建索引时，您可以进行与手动创建时相同级别的自定义。
And we can also swap out the vector store here for a different type of vector store.

144
00:12:15,000 --> 00:12:17,000
在这个笔记本中，我们使用了 stuff 方法。
So there's the same level of customization that you did when you created it by hand that's also available when you create the index here.

145
00:12:17,000 --> 00:12:19,000
stuff 方法非常好，因为它非常简单。
We use the stuff method in this notebook.

146
00:12:19,000 --> 00:12:25,000
您只需将所有内容放入一个提示符中，然后将其发送到语言模型并获取一个响应。
The stuff method is really nice because it's pretty simple.

147
00:12:25,000 --> 00:12:27,000
因此，很容易理解正在发生什么。
You just put all of it into one prompt and send that to the language model and get back one response.

148
00:12:27,000 --> 00:12:30,000
它非常便宜，而且效果很好。
So it's quite simple to understand what's going on.

149
00:12:30,000 --> 00:12:32,000
但是，这并不总是可以正常工作。
It's quite cheap and it works pretty well.

150
00:12:32,000 --> 00:12:37,000
因此，如果您记得，在笔记本中获取文档时，我们只返回了四个文档。
But that doesn't always work okay.

151
00:12:37,000 --> 00:12:39,000
它们相对较小。
So if you remember, when we fetched the documents in the notebook, we only got four documents back.

152
00:12:39,000 --> 00:12:44,000
但是，如果您想在许多不同类型的块上执行相同类型的问答，该怎么办？
And they were relatively small.

153
00:12:44,000 --> 00:12:47,000
那么我们可以使用几种不同的方法。
 
But what if you wanted to do the same type of question answering over lots of different types of chunks?

154
00:12:47,000 --> 00:12:48,000
第一个是Map Reduce。
Then there are a few different methods that we can use.

155
00:12:48,000 --> 00:12:55,000
这基本上是将所有块与问题一起传递给语言模型，获取回复，
The first is map reduce.

156
00:12:55,000 --> 00:13:02,000
然后使用另一个语言模型调用将所有单独的回复总结成最终答案。
This basically takes all the chunks, passes them along with the question to a language model, gets back a response,

157
00:13:02,000 --> 00:13:06,000
这非常强大，因为它可以在任意数量的文档上运行。
and then uses another language model call to summarize all of the individual responses into a final answer.

158
00:13:06,000 --> 00:13:11,000
而且它也非常强大，因为您可以并行处理单个问题。
This is really powerful because it can operate over any number of documents.

159
00:13:11,000 --> 00:13:13,000
但是它需要更多的调用。
And it's also really powerful because you can do the individual questions in parallel.

160
00:13:13,000 --> 00:13:19,000
它将所有文档视为独立的，这可能并不总是最理想的事情。
But it does take a lot more calls.

161
00:13:19,000 --> 00:13:24,000
Refine是另一种方法，再次用于循环许多文档。
And it does treat all the documents as independent, which may not always be the most desired thing.

162
00:13:24,000 --> 00:13:25,000
但它实际上是迭代的。
Refine, which is another method, is again used to loop over many documents.

163
00:13:25,000 --> 00:13:28,000
它建立在先前文档的答案之上。
But it actually does it iteratively.

164
00:13:28,000 --> 00:13:33,000
因此，这非常适合组合信息并随时间逐步构建答案。
It builds upon the answer from the previous document.

165
00:13:33,000 --> 00:13:36,000
它通常会导致更长的答案。
So this is really good for combining information and building up an answer over time.

166
00:13:36,000 --> 00:13:39,000
而且它也不太快，因为现在调用不是独立的。
It will generally lead to longer answers.

167
00:13:39,000 --> 00:13:43,000
它们依赖于先前调用的结果。
And it's also not as fast because now the calls aren't independent.

168
00:13:43,000 --> 00:13:49,000
这意味着它通常需要更长的时间，并且基本上需要与Map Reduce一样多的调用。
They depend on the result of previous calls.

169
00:13:49,000 --> 00:13:57,000
Map Re-rank是一种相当有趣且更为实验性的方法，其中您对每个文档进行单个语言模型调用。
This means that it often takes a good while longer and takes just as many calls as map reduce, basically.

170
00:13:57,000 --> 00:14:00,000
然后您还要求它返回一个分数。
Map re-rank is a pretty interesting and a bit more experimental one where you do a single call to the language model for each document.

171
00:14:00,000 --> 00:14:02,000
然后您选择最高分。
And you also ask it to return a score.

172
00:14:02,000 --> 00:14:06,000
这依赖于语言模型知道分数应该是什么。
And then you select the highest score.

173
00:14:06,000 --> 00:14:12,000
因此，您经常需要告诉它，嘿，如果它与文档相关，则应该是高分，并在那里精细调整说明。
This relies on the language model to know what the score should be.

174
00:14:12,000 --> 00:14:15,000
与Map Reduce类似，所有调用都是独立的。
So you often have to tell it, hey, it should be a high score if it's relevant to the document and really refine the instructions there.

175
00:14:15,000 --> 00:14:16,000
所以您可以批量处理它们。
Similar to map reduce, all the calls are independent.

176
00:14:16,000 --> 00:14:18,000
而且它相对较快。
So you can batch them.

177
00:14:18,000 --> 00:14:20,000
但是，您正在进行大量的语言模型调用。
And it's relatively fast.

178
00:14:20,000 --> 00:14:22,000
因此，它会更加昂贵。
But again, you're making a bunch of language model calls.

179
00:14:22,000 --> 00:14:29,000
这些方法中最常见的是Stuff方法，我们在笔记本中使用它将所有内容组合成一个文档。
So it will be a bit more expensive.

180
00:14:29,000 --> 00:14:35,000
第二种最常见的方法是Map Reduce方法，它将这些块发送到语言模型。
 
The most common of these methods is the stuff method, which we used in the notebook to combine it all into one document.

181
00:14:35,000 --> 00:14:42,000
这里的这些方法，如stuff、map reduce、refine和re-rank，也可以用于除了问答之外的许多其他链。
The second most common is the map reduce method, which takes these chunks and sends them to the language model.

182
00:14:42,000 --> 00:14:53,000
例如，map reduce链的一个非常常见的用例是摘要，其中您有一个非常长的文档，您想要递归地摘要其中的信息片段。
These methods here, stuff, map reduce, refine, and re-rank can also be used for lots of other chains besides just question answering.

183
00:14:53,000 --> 00:14:56,000
这就是关于文档问答的全部内容。
For example, a really common use case of the map reduce chain is for summarization, where you have a really long document and you want to recursively summarize pieces of information in it.

184
00:14:56,000 --> 00:15:00,000
正如您可能已经注意到的那样，我们这里有许多不同的链条。
That's it for question answering over documents.

185
00:15:00,000 --> 00:15:12,000
因此，在下一节中，我们将介绍更好地了解所有这些链条内部究竟发生了什么的方法。
As you may have noticed, there's a lot going on in the different chains that we have here.
